# Develop a surveillance pipeline to optimise multiple data sources

## Background

Once the scale of the COVID-19 pandemic became clear, the UK Government convened the [Scientific Advisory Group for Emergencies](https://www.gov.uk/government/organisations/scientific-advisory-group-for-emergencies) (SAGE). SAGE is [comprised of](https://www.gov.uk/government/publications/scientific-advisory-group-for-emergencies-sage-coronavirus-covid-19-response-membership/list-of-participants-of-sage-and-related-sub-groups) multiple working groups, the largest of which are the Scientific Pandemic Insights Group on Behaviours (SPI-B) and the Scientific Pandemic Influenza Group on Modelling (SPI-M). SPI-M ran complex computation models to advise the government on possible future trajectories of COVID-19 and how various interventions may affect the course of the virus. In contrast, SPI-B performed behavioural science experiments and analysis. These working groups remained separate within SAGE and there was minimal interaction between the two. However, closer monitoring of non-traditional ‘big data’ sources, such as behavioural data, retrospective analyses, and qualitative analyses, may improve predictive ability and policymaking for future disease outbreaks.

## Challenge

Build tools to integrate data from multiple sources for case-based surveillance. Use the provided datasets to demonstrate how quantitative and qualitative data can be combined to increase the predictive potential of models to better inform public health response. This should cover everything from integrating with APIs, data aggregation, modelling, all the way to public communication. You may consider anything from NLP for qualitative analysis and incorporation into a quantitative machine learning model, to guidelines for communicating qualitative data alongside quantitative data without diminishing the importance of both forms of data, to causal analysis to understand how quantitative and qualitative data affect each other. Your platform should be scalable and extensible and in the long run, (for instance, developed after the Ideathon) should be able to incorporate multiple quantitative and qualitative data sources such as interview data, behavioural analysis, genomic data, climate data, and more. Your platform should also allow pipelines to be created for people without coding experience and pipelines using public APIs should be sharable (with creators being citable). Finally, you should consider how your pipeline could incorporate, and adapt to, new data should there be changes in external factors, for example it was observed during the pandemic that when models forecasted high-levels of infections then people would act more cautiously thereby ensuring the forecasts did not come true – how would you incorporate this in your model (note you do not need to consider this exact scenario)?

## Questions to answer in your presentation

1. How do you propose to transparently evaluate your model and to demonstrate its performance (good or bad) over time?
2. In the long run, how would you engage with the public to ensure that forecasts from the model are not viewed as scientific jargon or overly pessimistic?
3. How would you broaden your prototype to include data from other sources ranging from quantitative databases to more social media?
